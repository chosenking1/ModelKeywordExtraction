{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [],
   "source": [
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing\n",
    "\n",
    "df = pd.read_csv('papers.csv')\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\ADMIN\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\ADMIN\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     C:\\Users\\ADMIN\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "\n",
    "\n",
    "\n",
    "stop_words = set(stopwords.words('english'))\n",
    "##Creating a list of custom stopwords\n",
    "new_words = [\"fig\",\"figure\",\"image\",\"sample\",\"using\",\n",
    "             \"show\", \"result\", \"large\",\n",
    "             \"also\", \"one\", \"two\", \"three\",\n",
    "             \"four\", \"five\", \"seven\",\"eight\",\"nine\"]\n",
    "stop_words = list(stop_words.union(new_words))\n",
    "\n",
    "\n",
    "def pre_process(text):\n",
    "    # lowercase\n",
    "    text = text.lower()\n",
    "\n",
    "    #remove tags\n",
    "    text = re.sub(\"&lt;/?.*?&gt;\", \" &lt;&gt; \", text)\n",
    "\n",
    "    # remove special characters and digits\n",
    "    text = re.sub(\"(\\\\d|\\\\W)+\", \" \", text)\n",
    "\n",
    "    ##Convert to list from string\n",
    "    text = text.split()\n",
    "\n",
    "    # remove stopwords\n",
    "    text = [word for word in text if word not in stop_words]\n",
    "\n",
    "    # remove words less than three letters\n",
    "    text = [word for word in text if len(word) >= 3]\n",
    "\n",
    "    # lemmatize\n",
    "    lmtzr = WordNetLemmatizer()\n",
    "    text = [lmtzr.lemmatize(word) for word in text]\n",
    "\n",
    "    return ' '.join(text)\n",
    "\n",
    "\n",
    "docs = df['paper_text'].apply(lambda x: pre_process(x))\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "#docs = docs.tolist()\n",
    "#create a vocabulary of words,\n",
    "cv=CountVectorizer(max_df=0.95,         # ignore words that appear in 95% of documents\n",
    "                   max_features=10000,  # the size of the vocabulary\n",
    "                   ngram_range=(1,3)    # vocabulary contains single words, bigrams, trigrams\n",
    "                  )\n",
    "word_count_vector=cv.fit_transform(docs)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "data": {
      "text/plain": "TfidfTransformer()",
      "text/html": "<style>#sk-container-id-1 {color: black;background-color: white;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>TfidfTransformer()</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">TfidfTransformer</label><div class=\"sk-toggleable__content\"><pre>TfidfTransformer()</pre></div></div></div></div></div>"
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "\n",
    "tfidf_transformer=TfidfTransformer(smooth_idf=True,use_idf=True)\n",
    "tfidf_transformer.fit(word_count_vector)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=====Title=====\n",
      "Algorithms for Non-negative Matrix Factorization\n",
      "\n",
      "=====Abstract=====\n",
      "Non-negative matrix factorization (NMF) has previously been shown to \r\n",
      "be a useful decomposition for multivariate data. Two different multi- \r\n",
      "plicative algorithms for NMF are analyzed. They differ only slightly in \r\n",
      "the multiplicative factor used in the update rules. One algorithm can be \r\n",
      "shown to minimize the conventional least squares error while the other \r\n",
      "minimizes the generalized Kullback-Leibler divergence. The monotonic \r\n",
      "convergence of both algorithms can be proven using an auxiliary func- \r\n",
      "tion analogous to that used for proving convergence of the Expectation- \r\n",
      "Maximization algorithm. The algorithms can also be interpreted as diag- \r\n",
      "onally rescaled gradient descent, where the rescaling factor is optimally \r\n",
      "chosen to ensure convergence. \n",
      "\n",
      "===Keywords===\n",
      "update rule 0.344\n",
      "update 0.285\n",
      "auxiliary 0.212\n",
      "non negative matrix 0.21\n",
      "negative matrix 0.209\n",
      "rule 0.192\n",
      "nmf 0.183\n",
      "multiplicative 0.175\n",
      "matrix factorization 0.163\n",
      "matrix 0.163\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ADMIN\\PycharmProjects\\keyWordExtractionProject\\venv\\lib\\site-packages\\sklearn\\utils\\deprecation.py:87: FutureWarning: Function get_feature_names is deprecated; get_feature_names is deprecated in 1.0 and will be removed in 1.2. Please use get_feature_names_out instead.\n",
      "  warnings.warn(msg, category=FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "def sort_coo(coo_matrix):\n",
    "    tuples = zip(coo_matrix.col, coo_matrix.data)\n",
    "    return sorted(tuples, key=lambda x: (x[1], x[0]), reverse=True)\n",
    "\n",
    "def extract_topn_from_vector(feature_names, sorted_items, topn=10):\n",
    "    \"\"\"get the feature names and tf-idf score of top n items\"\"\"\n",
    "\n",
    "    #use only topn items from vector\n",
    "    sorted_items = sorted_items[:topn]\n",
    "\n",
    "    score_vals = []\n",
    "    feature_vals = []\n",
    "\n",
    "    for idx, score in sorted_items:\n",
    "        fname = feature_names[idx]\n",
    "\n",
    "        #keep track of feature name and its corresponding score\n",
    "        score_vals.append(round(score, 3))\n",
    "        # feature_vals.append(feature_names[idx])\n",
    "        feature_vals.append(fname)\n",
    "\n",
    "    #create a tuples of feature,score\n",
    "    #results = zip(feature_vals,score_vals)\n",
    "    results= {}\n",
    "    for idx in range(len(feature_vals)):\n",
    "        results[feature_vals[idx]]=score_vals[idx]\n",
    "\n",
    "    return results\n",
    "\n",
    "# get feature names\n",
    "feature_names=cv.get_feature_names()\n",
    "\n",
    "def get_keywords(idx, statement):\n",
    "\n",
    "    #generate tf-idf for the given document\n",
    "    tf_idf_vector=tfidf_transformer.transform(cv.transform([statement[idx]]))\n",
    "\n",
    "    #sort the tf-idf vectors by descending order of scores\n",
    "    sorted_items=sort_coo(tf_idf_vector.tocoo())\n",
    "\n",
    "    #extract only the top n; n here is 10\n",
    "    keywords=extract_topn_from_vector(feature_names,sorted_items,10)\n",
    "\n",
    "    return keywords\n",
    "\n",
    "def print_results(idx,keywords, df):\n",
    "    # now print the results\n",
    "    print(\"\\n=====Title=====\")\n",
    "    print(df['title'][idx])\n",
    "    print(\"\\n=====Abstract=====\")\n",
    "    print(df['abstract'][idx])\n",
    "    print(\"\\n===Keywords===\")\n",
    "    for k in keywords:\n",
    "        # print(k,keywords[k])\n",
    "        print(k)\n",
    "idx=941\n",
    "keywords=get_keywords(idx, docs)\n",
    "print_results(idx,keywords, df)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "import pickle\n",
    "# pickle_out = open(\"predict.pkl\",\"wb\")\n",
    "# pickle.dump(get_keywords, pickle_out)\n",
    "# pickle_out.close()\n",
    "keywords_prediction = get_keywords(941,\"\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=====Title=====\n",
      "Mine home\n",
      "\n",
      "=====Abstract=====\n",
      "Temi gjfhgdsghs\n",
      "\n",
      "===Keywords===\n",
      "design 0.549\n",
      "composition 0.257\n",
      "class 0.244\n",
      "designed 0.234\n",
      "code 0.221\n",
      "classified 0.218\n",
      "es 0.185\n",
      "creation 0.159\n",
      "quite 0.144\n",
      "one 0.143\n"
     ]
    }
   ],
   "source": [
    "document = { \"title\":[\"Mine home\"], \"abstract\":[\"Temi gjfhgdsghs\"],\"paper_text\":[\"What is a Design Pattern? Design patterns are design level solutions for recurring problems that we software engineers come across often. It’s not code - I repeat, ❌CODE. It is like a description on how to tackle these problems and design a solution.Using these patterns is considered good practice, as the design of the solution is quite tried and tested, resulting in higher readability of the final code. Design patterns are quite often created for and used by OOP Languages, like Java, in which most of the examples from here on will be written.Types of design patternsThere are about 26 Patterns currently discovered (I hardly think I will do them all…).These 26 can be classified into 3 types:1. Creational: These patterns are designed for class instantiation. They can be either class-creation patterns or object-creational patterns.2. Structural: These patterns are designed with regard to a class's structure and composition. The main goal of most of these patterns is to increase the functionality of the class(es) involved, without changing much of its composition.3. Behavioral: These patterns are designed depending on how one class communicates with others.In this post, we will go through one basic design pattern for each classified type.\"]}\n",
    "test = document['paper_text']\n",
    "# pre_process(document[0])\n",
    "\n",
    "# get_keywords(0,test)\n",
    "print_results(0,get_keywords(0,test),document)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "outputs": [
    {
     "data": {
      "text/plain": "{'design': 0.549,\n 'composition': 0.257,\n 'class': 0.244,\n 'designed': 0.234,\n 'code': 0.221,\n 'classified': 0.218,\n 'es': 0.185,\n 'creation': 0.159,\n 'quite': 0.144,\n 'one': 0.143}"
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\n",
    "pickle.dump(document,open('generate.pkl','wb'))\n",
    "\n",
    "text = document['paper_text']\n",
    "get_keywords(0,text)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "outputs": [],
   "source": [
    "document = pickle.load(open('generate.pkl', 'rb'))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open('testing.pkl','wb')as f:\n",
    "    pickle.dump(testing,f)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}